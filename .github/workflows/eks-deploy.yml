name: ‚ò∏Ô∏è EKS Deploy

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy'
        required: true
        type: choice
        options:
          - stg
          - prd
        default: 'stg'
      action:
        description: 'Action to perform'
        required: true
        type: choice
        options:
          - plan
          - provision
          - deploy
          - destroy
        default: 'deploy'

env:
  AWS_REGION: us-east-1
  TERRAFORM_VERSION: 1.6.0
  KUBECTL_VERSION: 1.32.0

jobs:
  eks-plan:
    if: inputs.action == 'plan'
    name: Terraform Plan (EKS Resources)
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Terraform Init
      working-directory: terraform/${{ inputs.environment }}
      run: terraform init

    - name: Terraform Plan (EKS Resources Only)
      working-directory: terraform/${{ inputs.environment }}
      run: |
        # Plan all resources - enable_eks controls what gets created
        terraform plan

  eks-provision:
    if: inputs.action == 'provision'
    name: Provision EKS Cluster
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Terraform Init
      working-directory: terraform/${{ inputs.environment }}
      run: terraform init

    - name: Terraform Apply (EKS Resources Only)
      working-directory: terraform/${{ inputs.environment }}
      run: |
        # Apply all resources - enable_eks controls what gets created
        terraform apply -auto-approve

    - name: Get EKS cluster info
      id: eks-info
      run: |
        # Wait a few seconds for cluster to be fully registered
        sleep 10
        
        CLUSTER_NAME="tx01-eks-${{ inputs.environment }}"
        echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT

        # Wait for cluster to be ACTIVE
        echo "Waiting for cluster to be ACTIVE..."
        aws eks wait cluster-active --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
        
        CLUSTER_ENDPOINT=$(aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} --query 'cluster.endpoint' --output text)
        echo "cluster_endpoint=$CLUSTER_ENDPOINT" >> $GITHUB_OUTPUT

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig \
          --name ${{ steps.eks-info.outputs.cluster_name }} \
          --region ${{ env.AWS_REGION }}

    - name: Install AWS Load Balancer Controller
      run: |
        # Add Helm repo
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update

        # Get cluster OIDC provider
        OIDC_PROVIDER=$(aws eks describe-cluster --name ${{ steps.eks-info.outputs.cluster_name }} --region ${{ env.AWS_REGION }} --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///")

        # Get IAM role ARN
        ROLE_ARN=$(aws iam get-role --role-name tx01-alb-controller-${{ inputs.environment }} --query 'Role.Arn' --output text)

        # Install controller with version compatible with Kubernetes 1.32
        helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
          --version 1.10.1 \
          --namespace kube-system \
          --set clusterName=${{ steps.eks-info.outputs.cluster_name }} \
          --set serviceAccount.create=true \
          --set serviceAccount.name=aws-load-balancer-controller \
          --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=$ROLE_ARN \
          --set region=${{ env.AWS_REGION }} \
          --set vpcId=$(aws eks describe-cluster --name ${{ steps.eks-info.outputs.cluster_name }} --region ${{ env.AWS_REGION }} --query "cluster.resourcesVpcConfig.vpcId" --output text)

    - name: Wait for controller
      run: |
        kubectl wait --namespace kube-system \
          --for=condition=ready pod \
          --selector=app.kubernetes.io/name=aws-load-balancer-controller \
          --timeout=180s

    - name: Configure RDS Security Group for EKS access
      run: |
        echo "üîê Configuring RDS Security Group to allow EKS cluster access..."
        
        # Get EKS cluster security group
        EKS_SG=$(aws eks describe-cluster \
          --name ${{ steps.eks-info.outputs.cluster_name }} \
          --region ${{ env.AWS_REGION }} \
          --query 'cluster.resourcesVpcConfig.clusterSecurityGroupId' \
          --output text)
        
        echo "‚úÖ EKS Cluster Security Group: $EKS_SG"
        
        # Get RDS instance
        RDS_INSTANCE="tx01-db-${{ inputs.environment }}"
        
        # Check if RDS exists
        RDS_EXISTS=$(aws rds describe-db-instances \
          --region ${{ env.AWS_REGION }} \
          --db-instance-identifier $RDS_INSTANCE \
          --query 'DBInstances[0].DBInstanceIdentifier' \
          --output text 2>/dev/null || echo "NOT_FOUND")
        
        if [ "$RDS_EXISTS" = "NOT_FOUND" ]; then
          echo "‚ö†Ô∏è  RDS instance not found, skipping security group configuration"
          exit 0
        fi
        
        # Get RDS security group
        RDS_SG=$(aws rds describe-db-instances \
          --region ${{ env.AWS_REGION }} \
          --db-instance-identifier $RDS_INSTANCE \
          --query 'DBInstances[0].VpcSecurityGroups[0].VpcSecurityGroupId' \
          --output text)
        
        echo "‚úÖ RDS Security Group: $RDS_SG"
        
        # Check if rule already exists
        RULE_EXISTS=$(aws ec2 describe-security-groups \
          --region ${{ env.AWS_REGION }} \
          --group-ids $RDS_SG \
          --query "SecurityGroups[0].IpPermissions[?FromPort==\`5432\` && ToPort==\`5432\`].UserIdGroupPairs[?GroupId==\`$EKS_SG\`].GroupId" \
          --output text)
        
        if [ -n "$RULE_EXISTS" ]; then
          echo "‚úÖ Security group rule already exists (EKS ‚Üí RDS)"
        else
          echo "üìù Adding security group rule (EKS ‚Üí RDS)..."
          aws ec2 authorize-security-group-ingress \
            --region ${{ env.AWS_REGION }} \
            --group-id $RDS_SG \
            --ip-permissions "IpProtocol=tcp,FromPort=5432,ToPort=5432,UserIdGroupPairs=[{GroupId=$EKS_SG,Description='PostgreSQL from EKS cluster nodes'}]"
          
          echo "‚úÖ Security group rule added successfully"
        fi
        
        echo ""
        echo "üéØ RDS is now accessible from EKS cluster pods"

    - name: Check if Metrics Server should be installed
      id: check-metrics
      run: |
        # Get instance type from Terraform tfvars
        INSTANCE_TYPE=$(grep "eks_node_instance_type" terraform/${{ inputs.environment }}/terraform.tfvars | awk -F'"' '{print $2}')
        echo "instance_type=$INSTANCE_TYPE" >> $GITHUB_OUTPUT
        
        # Skip metrics server for t3.micro due to pod density limitations
        if [ "$INSTANCE_TYPE" == "t3.micro" ]; then
          echo "should_install=false" >> $GITHUB_OUTPUT
          echo "‚ö†Ô∏è Skipping Metrics Server installation for t3.micro (pod density constraints)"
        else
          echo "should_install=true" >> $GITHUB_OUTPUT
          echo "‚úÖ Will install Metrics Server for $INSTANCE_TYPE"
        fi

    - name: Install Metrics Server
      if: steps.check-metrics.outputs.should_install == 'true'
      run: |
        kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
        
        # Wait for metrics server to be ready
        kubectl wait --namespace kube-system \
          --for=condition=ready pod \
          --selector=k8s-app=metrics-server \
          --timeout=120s

    outputs:
      cluster_name: ${{ steps.eks-info.outputs.cluster_name }}
      cluster_endpoint: ${{ steps.eks-info.outputs.cluster_endpoint }}

  eks-deploy:
    if: inputs.action == 'deploy'
    name: Deploy Application to EKS
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Install kubectl
      run: |
        curl -LO "https://dl.k8s.io/release/v${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/

    - name: Get EKS cluster name
      id: eks-info
      run: |
        CLUSTER_NAME=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?contains(@, 'tx01-eks-${{ inputs.environment }}')]" --output text)
        echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT

    - name: Check if EKS cluster exists
      run: |
        if [ -z "${{ steps.eks-info.outputs.cluster_name }}" ]; then
          echo "EKS cluster not found, skipping deployment"
          exit 0
        fi

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig \
          --name ${{ steps.eks-info.outputs.cluster_name }} \
          --region ${{ env.AWS_REGION }}

    - name: Get database credentials from Secrets Manager
      id: db-creds
      run: |
        SECRET_VALUE=$(aws secretsmanager get-secret-value \
          --secret-id tx01-db-credentials-${{ inputs.environment }}-v2 \
          --region ${{ env.AWS_REGION }} \
          --query SecretString \
          --output text)

        echo "db_host=$(echo $SECRET_VALUE | jq -r '.host')" >> $GITHUB_OUTPUT
        echo "db_port=$(echo $SECRET_VALUE | jq -r '.port')" >> $GITHUB_OUTPUT
        echo "db_name=$(echo $SECRET_VALUE | jq -r '.dbname')" >> $GITHUB_OUTPUT
        echo "db_user=$(echo $SECRET_VALUE | jq -r '.username')" >> $GITHUB_OUTPUT
        echo "::add-mask::$(echo $SECRET_VALUE | jq -r '.password')"
        echo "db_password=$(echo $SECRET_VALUE | jq -r '.password')" >> $GITHUB_OUTPUT

    - name: Get ECR repository URL
      id: ecr
      run: |
        REPO_URL=$(aws ecr describe-repositories \
          --repository-names dx01-app \
          --region ${{ env.AWS_REGION }} \
          --query 'repositories[0].repositoryUri' \
          --output text)
        echo "repository_url=$REPO_URL" >> $GITHUB_OUTPUT

    - name: Get latest image tag
      id: image
      run: |
        LATEST_TAG=$(aws ecr describe-images \
          --repository-name dx01-app \
          --region ${{ env.AWS_REGION }} \
          --query 'sort_by(imageDetails,& imagePushedAt)[-1].imageTags[0]' \
          --output text)
        echo "tag=$LATEST_TAG" >> $GITHUB_OUTPUT

    - name: Create ECR registry secret
      run: |
        kubectl create secret docker-registry ecr-registry-secret \
          --docker-server=${{ steps.ecr.outputs.repository_url }} \
          --docker-username=AWS \
          --docker-password=$(aws ecr get-login-password --region ${{ env.AWS_REGION }}) \
          --namespace=default \
          --dry-run=client -o yaml | kubectl apply -f -

    - name: Create database credentials secret
      run: |
        export DB_HOST="${{ steps.db-creds.outputs.db_host }}"
        export DB_PORT="${{ steps.db-creds.outputs.db_port }}"
        export DB_NAME="${{ steps.db-creds.outputs.db_name }}"
        export DB_USER="${{ steps.db-creds.outputs.db_user }}"
        export DB_PASSWORD="${{ steps.db-creds.outputs.db_password }}"

        envsubst < k8s/secret.yaml | kubectl apply -f -

    - name: Deploy Service Account
      run: |
        # Create service account without IRSA role for now
        kubectl create serviceaccount tx01-service-account --dry-run=client -o yaml | kubectl apply -f -

    - name: Deploy application
      run: |
        export ECR_REPOSITORY_URL="${{ steps.ecr.outputs.repository_url }}"
        export IMAGE_TAG="${{ steps.image.outputs.tag }}"
        export ENVIRONMENT="${{ inputs.environment }}"

        envsubst < k8s/deployment.yaml | kubectl apply -f -
        envsubst < k8s/service.yaml | kubectl apply -f -

    - name: Deploy Ingress
      run: |
        export ENVIRONMENT="${{ inputs.environment }}"

        envsubst < k8s/ingress.yaml | kubectl apply -f -

    - name: Check if HPA should be deployed
      id: check-hpa
      run: |
        # Check if metrics-server is running
        METRICS_SERVER=$(kubectl get deployment metrics-server -n kube-system --ignore-not-found -o name)
        
        if [ -z "$METRICS_SERVER" ]; then
          echo "should_deploy=false" >> $GITHUB_OUTPUT
          echo "‚ö†Ô∏è Skipping HPA deployment (Metrics Server not available for t3.micro)"
        else
          echo "should_deploy=true" >> $GITHUB_OUTPUT
          echo "‚úÖ Will deploy HPA (Metrics Server available)"
        fi

    - name: Deploy HPA
      if: steps.check-hpa.outputs.should_deploy == 'true'
      run: kubectl apply -f k8s/hpa.yaml

    - name: Wait for deployment
      id: rollout
      continue-on-error: true
      run: |
        kubectl rollout status deployment/tx01-app --timeout=5m

    - name: Debug deployment status
      if: always()
      run: |
        echo "### Deployment Status ###"
        kubectl get deployments -o wide
        echo ""
        echo "### ReplicaSets ###"
        kubectl get replicasets -o wide
        echo ""
        echo "### Pods ###"
        kubectl get pods -o wide
        echo ""
        echo "### Pod Descriptions ###"
        kubectl describe pods
        echo ""
        echo "### Events ###"
        kubectl get events --sort-by=.metadata.creationTimestamp
        echo ""
        echo "### Services ###"
        kubectl get svc
        echo ""
        echo "### Ingress ###"
        kubectl get ingress
        echo ""
        echo "### HPA ###"
        kubectl get hpa

    - name: Get pod logs if rollout failed
      if: steps.rollout.outcome != 'success'
      continue-on-error: true
      run: |
        echo "### Pod Logs ###"
        for pod in $(kubectl get pods -l app=tx01 -o name); do
          echo "Logs for $pod:"
          kubectl logs $pod --tail=50 || echo "Could not get logs for $pod"
          echo "---"
        done

    - name: Check if rollout succeeded
      if: steps.rollout.outcome != 'success'
      run: |
        echo "::error::Deployment rollout failed. Check Debug deployment status step for details."
        exit 1

    - name: Get ALB DNS
      run: |
        ALB_DNS=$(aws elbv2 describe-load-balancers \
          --region ${{ env.AWS_REGION }} \
          --query "LoadBalancers[?contains(LoadBalancerName, 'tx01-alb-${{ inputs.environment }}')].DNSName" \
          --output text)
        echo "Application URL: http://$ALB_DNS"

  eks-destroy:
    if: inputs.action == 'destroy'
    name: Destroy EKS Cluster
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Install kubectl
      run: |
        curl -LO "https://dl.k8s.io/release/v${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/

    - name: Get EKS cluster name
      id: eks-info
      run: |
        CLUSTER_NAME=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?contains(@, 'tx01-eks-${{ inputs.environment }}')]" --output text || echo "")
        echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT

    - name: Clean up Kubernetes resources
      if: steps.eks-info.outputs.cluster_name != ''
      run: |
        aws eks update-kubeconfig \
          --name ${{ steps.eks-info.outputs.cluster_name }} \
          --region ${{ env.AWS_REGION }}

        # Delete all resources
        kubectl delete ingress --all -n default || true
        kubectl delete hpa --all -n default || true
        kubectl delete deployment --all -n default || true
        kubectl delete service --all -n default || true
        kubectl delete secret --all -n default || true

        # Uninstall Load Balancer Controller
        helm uninstall aws-load-balancer-controller -n kube-system || true

        # Wait for resources to be deleted
        sleep 30

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Terraform Init
      working-directory: terraform/${{ inputs.environment }}
      run: terraform init

    - name: Terraform Destroy EKS Resources
      working-directory: terraform/${{ inputs.environment }}
      run: |
        # Destroy only EKS-related resources without affecting EC2
        terraform destroy -auto-approve \
          -target=module.infrastructure.kubernetes_config_map_v1_data.aws_auth \
          -target=module.infrastructure.aws_eks_addon.coredns \
          -target=module.infrastructure.aws_eks_addon.kube_proxy \
          -target=module.infrastructure.aws_eks_addon.vpc_cni \
          -target=module.infrastructure.aws_eks_node_group.main \
          -target=module.infrastructure.aws_eks_cluster.main \
          -target=module.infrastructure.aws_iam_role.eks_cluster \
          -target=module.infrastructure.aws_iam_role.eks_node \
          -target=module.infrastructure.aws_iam_role_policy_attachment.eks_cluster_policy \
          -target=module.infrastructure.aws_iam_role_policy_attachment.eks_node_policy \
          -target=module.infrastructure.aws_iam_role_policy_attachment.eks_cni_policy \
          -target=module.infrastructure.aws_iam_role_policy_attachment.eks_container_registry_policy \
          -target=module.infrastructure.aws_security_group.eks_cluster \
          -target=module.infrastructure.aws_cloudwatch_log_group.eks_cluster

    - name: Verify destruction
      run: |
        CLUSTER_EXISTS=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?contains(@, 'tx01-eks-${{ inputs.environment }}')]" --output text || echo "")
        if [ -z "$CLUSTER_EXISTS" ]; then
          echo "‚úÖ EKS cluster successfully destroyed"
        else
          echo "‚ö†Ô∏è EKS cluster still exists"
          exit 1
        fi
