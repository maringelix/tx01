name: ☸️ EKS Deploy

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy'
        required: true
        type: choice
        options:
          - stg
          - prd
        default: 'stg'
      action:
        description: 'Action to perform'
        required: true
        type: choice
        options:
          - provision
          - deploy
          - destroy
        default: 'deploy'

env:
  AWS_REGION: us-east-1
  TERRAFORM_VERSION: 1.6.0
  KUBECTL_VERSION: 1.28.0

jobs:
  eks-provision:
    if: inputs.action == 'provision'
    name: Provision EKS Cluster
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Terraform Init
      working-directory: terraform/${{ inputs.environment }}
      run: terraform init

    - name: Terraform Plan (Enable EKS)
      working-directory: terraform/${{ inputs.environment }}
      run: |
        terraform plan \
          -var="enable_eks=true" \
          -out=tfplan

    - name: Terraform Apply
      working-directory: terraform/${{ inputs.environment }}
      run: terraform apply -auto-approve tfplan

    - name: Get EKS cluster info
      id: eks-info
      run: |
        CLUSTER_NAME=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?contains(@, 'tx01-eks-${{ inputs.environment }}')]" --output text)
        echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
        
        CLUSTER_ENDPOINT=$(aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} --query 'cluster.endpoint' --output text)
        echo "cluster_endpoint=$CLUSTER_ENDPOINT" >> $GITHUB_OUTPUT

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig \
          --name ${{ steps.eks-info.outputs.cluster_name }} \
          --region ${{ env.AWS_REGION }}

    - name: Install AWS Load Balancer Controller
      run: |
        # Add Helm repo
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update
        
        # Get cluster OIDC provider
        OIDC_PROVIDER=$(aws eks describe-cluster --name ${{ steps.eks-info.outputs.cluster_name }} --region ${{ env.AWS_REGION }} --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///")
        
        # Get IAM role ARN
        ROLE_ARN=$(aws iam get-role --role-name tx01-alb-controller-${{ inputs.environment }} --query 'Role.Arn' --output text)
        
        # Install controller
        helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
          --namespace kube-system \
          --set clusterName=${{ steps.eks-info.outputs.cluster_name }} \
          --set serviceAccount.create=true \
          --set serviceAccount.name=aws-load-balancer-controller \
          --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=$ROLE_ARN \
          --set region=${{ env.AWS_REGION }} \
          --set vpcId=$(aws eks describe-cluster --name ${{ steps.eks-info.outputs.cluster_name }} --region ${{ env.AWS_REGION }} --query "cluster.resourcesVpcConfig.vpcId" --output text)

    - name: Wait for controller
      run: |
        kubectl wait --namespace kube-system \
          --for=condition=ready pod \
          --selector=app.kubernetes.io/name=aws-load-balancer-controller \
          --timeout=180s

    outputs:
      cluster_name: ${{ steps.eks-info.outputs.cluster_name }}
      cluster_endpoint: ${{ steps.eks-info.outputs.cluster_endpoint }}

  eks-deploy:
    if: inputs.action == 'deploy'
    name: Deploy Application to EKS
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Install kubectl
      run: |
        curl -LO "https://dl.k8s.io/release/v${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/

    - name: Get EKS cluster name
      id: eks-info
      run: |
        CLUSTER_NAME=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?contains(@, 'tx01-eks-${{ inputs.environment }}')]" --output text)
        echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig \
          --name ${{ steps.eks-info.outputs.cluster_name }} \
          --region ${{ env.AWS_REGION }}

    - name: Get database credentials from Secrets Manager
      id: db-creds
      run: |
        SECRET_VALUE=$(aws secretsmanager get-secret-value \
          --secret-id tx01-db-credentials-${{ inputs.environment }}-v2 \
          --region ${{ env.AWS_REGION }} \
          --query SecretString \
          --output text)
        
        echo "db_host=$(echo $SECRET_VALUE | jq -r '.host')" >> $GITHUB_OUTPUT
        echo "db_port=$(echo $SECRET_VALUE | jq -r '.port')" >> $GITHUB_OUTPUT
        echo "db_name=$(echo $SECRET_VALUE | jq -r '.dbname')" >> $GITHUB_OUTPUT
        echo "db_user=$(echo $SECRET_VALUE | jq -r '.username')" >> $GITHUB_OUTPUT
        echo "::add-mask::$(echo $SECRET_VALUE | jq -r '.password')"
        echo "db_password=$(echo $SECRET_VALUE | jq -r '.password')" >> $GITHUB_OUTPUT

    - name: Get ECR repository URL
      id: ecr
      run: |
        REPO_URL=$(aws ecr describe-repositories \
          --repository-names tx01-nginx \
          --region ${{ env.AWS_REGION }} \
          --query 'repositories[0].repositoryUri' \
          --output text)
        echo "repository_url=$REPO_URL" >> $GITHUB_OUTPUT

    - name: Get latest image tag
      id: image
      run: |
        LATEST_TAG=$(aws ecr describe-images \
          --repository-name tx01-nginx \
          --region ${{ env.AWS_REGION }} \
          --query 'sort_by(imageDetails,& imagePushedAt)[-1].imageTags[0]' \
          --output text)
        echo "tag=$LATEST_TAG" >> $GITHUB_OUTPUT

    - name: Create ECR registry secret
      run: |
        kubectl create secret docker-registry ecr-registry-secret \
          --docker-server=${{ steps.ecr.outputs.repository_url }} \
          --docker-username=AWS \
          --docker-password=$(aws ecr get-login-password --region ${{ env.AWS_REGION }}) \
          --namespace=default \
          --dry-run=client -o yaml | kubectl apply -f -

    - name: Create database credentials secret
      run: |
        export DB_HOST="${{ steps.db-creds.outputs.db_host }}"
        export DB_PORT="${{ steps.db-creds.outputs.db_port }}"
        export DB_NAME="${{ steps.db-creds.outputs.db_name }}"
        export DB_USER="${{ steps.db-creds.outputs.db_user }}"
        export DB_PASSWORD="${{ steps.db-creds.outputs.db_password }}"
        
        envsubst < k8s/secret.yaml | kubectl apply -f -

    - name: Get ALB name
      id: alb
      run: |
        ALB_NAME=$(aws elbv2 describe-load-balancers \
          --region ${{ env.AWS_REGION }} \
          --query "LoadBalancers[?contains(LoadBalancerName, 'tx01-alb-${{ inputs.environment }}')].LoadBalancerName" \
          --output text)
        echo "name=$ALB_NAME" >> $GITHUB_OUTPUT

    - name: Deploy Service Account
      run: |
        # Get IRSA role ARN (se existir)
        IRSA_ROLE_ARN=$(aws iam list-roles --query "Roles[?RoleName=='tx01-eks-pod-role-${{ inputs.environment }}'].Arn" --output text || echo "")
        export IRSA_ROLE_ARN="${IRSA_ROLE_ARN}"
        
        if [ -n "$IRSA_ROLE_ARN" ]; then
          envsubst < k8s/serviceaccount.yaml | kubectl apply -f -
        else
          echo "No IRSA role found, skipping service account with role annotation"
        fi

    - name: Deploy application
      run: |
        export ECR_REPOSITORY_URL="${{ steps.ecr.outputs.repository_url }}"
        export IMAGE_TAG="${{ steps.image.outputs.tag }}"
        export ENVIRONMENT="${{ inputs.environment }}"
        
        envsubst < k8s/deployment.yaml | kubectl apply -f -
        envsubst < k8s/service.yaml | kubectl apply -f -

    - name: Deploy Ingress
      run: |
        export ALB_NAME="${{ steps.alb.outputs.name }}"
        export ENVIRONMENT="${{ inputs.environment }}"
        
        envsubst < k8s/ingress.yaml | kubectl apply -f -

    - name: Deploy HPA
      run: kubectl apply -f k8s/hpa.yaml

    - name: Wait for deployment
      id: rollout
      continue-on-error: true
      run: |
        kubectl rollout status deployment/tx01-app --timeout=5m

    - name: Debug deployment status
      if: always()
      run: |
        echo "### Deployment Status ###"
        kubectl get deployments -o wide
        echo ""
        echo "### ReplicaSets ###"
        kubectl get replicasets -o wide
        echo ""
        echo "### Pods ###"
        kubectl get pods -o wide
        echo ""
        echo "### Pod Descriptions ###"
        kubectl describe pods
        echo ""
        echo "### Events ###"
        kubectl get events --sort-by=.metadata.creationTimestamp
        echo ""
        echo "### Services ###"
        kubectl get svc
        echo ""
        echo "### Ingress ###"
        kubectl get ingress
        echo ""
        echo "### HPA ###"
        kubectl get hpa

    - name: Check if rollout succeeded
      if: steps.rollout.outcome == 'failure'
      run: |
        echo "::error::Deployment rollout failed. Check Debug deployment status step for details."
        exit 1

    - name: Get ALB DNS
      run: |
        ALB_DNS=$(aws elbv2 describe-load-balancers \
          --region ${{ env.AWS_REGION }} \
          --query "LoadBalancers[?contains(LoadBalancerName, 'tx01-alb-${{ inputs.environment }}')].DNSName" \
          --output text)
        echo "Application URL: http://$ALB_DNS"

  eks-destroy:
    if: inputs.action == 'destroy'
    name: Destroy EKS Cluster
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Install kubectl
      run: |
        curl -LO "https://dl.k8s.io/release/v${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/

    - name: Get EKS cluster name
      id: eks-info
      run: |
        CLUSTER_NAME=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?contains(@, 'tx01-eks-${{ inputs.environment }}')]" --output text || echo "")
        echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT

    - name: Clean up Kubernetes resources
      if: steps.eks-info.outputs.cluster_name != ''
      run: |
        aws eks update-kubeconfig \
          --name ${{ steps.eks-info.outputs.cluster_name }} \
          --region ${{ env.AWS_REGION }}
        
        # Delete all resources
        kubectl delete ingress --all -n default || true
        kubectl delete hpa --all -n default || true
        kubectl delete deployment --all -n default || true
        kubectl delete service --all -n default || true
        kubectl delete secret --all -n default || true
        
        # Uninstall Load Balancer Controller
        helm uninstall aws-load-balancer-controller -n kube-system || true
        
        # Wait for resources to be deleted
        sleep 30

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Terraform Init
      working-directory: terraform/${{ inputs.environment }}
      run: terraform init

    - name: Terraform Plan (Disable EKS)
      working-directory: terraform/${{ inputs.environment }}
      run: |
        terraform plan \
          -var="enable_eks=false" \
          -out=tfplan

    - name: Terraform Apply
      working-directory: terraform/${{ inputs.environment }}
      run: terraform apply -auto-approve tfplan

    - name: Verify destruction
      run: |
        CLUSTER_EXISTS=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?contains(@, 'tx01-eks-${{ inputs.environment }}')]" --output text || echo "")
        if [ -z "$CLUSTER_EXISTS" ]; then
          echo "✅ EKS cluster successfully destroyed"
        else
          echo "⚠️ EKS cluster still exists"
          exit 1
        fi
