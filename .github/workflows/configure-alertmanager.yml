name: ðŸ”” Configure AlertManager

on:
  workflow_dispatch:
    inputs:
      slack_channel:
        description: 'Slack channel name (without #)'
        required: true
        default: 'alerts'
      severity_filter:
        description: 'Minimum severity to alert'
        required: true
        type: choice
        options:
          - critical
          - warning
          - info
        default: 'warning'

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: tx01-eks-stg

jobs:
  configure-alertmanager:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Create AlertManager configuration
        run: |
          cat > /tmp/alertmanager-values.yaml <<'EOF'
          alertmanager:
            config:
              global:
                resolve_timeout: 5m
                slack_api_url: '${{ secrets.SLACK_WEBHOOK_URL }}'
              
              route:
                group_by: ['alertname', 'cluster', 'service']
                group_wait: 10s
                group_interval: 10s
                repeat_interval: 12h
                receiver: 'slack-notifications'
                routes:
                  - match:
                      severity: critical
                    receiver: 'slack-critical'
                    continue: true
                  - match:
                      severity: warning
                    receiver: 'slack-warnings'
                    continue: true
                  - match:
                      alertname: Watchdog
                    receiver: 'null'
              
              receivers:
                - name: 'null'
                
                - name: 'slack-notifications'
                  slack_configs:
                    - channel: '#${{ github.event.inputs.slack_channel }}'
                      send_resolved: true
                      title: 'ðŸ”” [{{ .Status | toUpper }}] {{ .CommonLabels.alertname }}'
                      text: |
                        {{ range .Alerts }}
                        *Alert:* {{ .Labels.alertname }}
                        *Severity:* {{ .Labels.severity }}
                        *Summary:* {{ .Annotations.summary }}
                        *Description:* {{ .Annotations.description }}
                        *Cluster:* ${{ env.CLUSTER_NAME }}
                        *Namespace:* {{ .Labels.namespace }}
                        {{ if .Labels.pod }}*Pod:* {{ .Labels.pod }}{{ end }}
                        {{ if .Labels.instance }}*Instance:* {{ .Labels.instance }}{{ end }}
                        {{ end }}
                      color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
                
                - name: 'slack-critical'
                  slack_configs:
                    - channel: '#${{ github.event.inputs.slack_channel }}'
                      send_resolved: true
                      title: 'ðŸš¨ [CRITICAL] {{ .CommonLabels.alertname }}'
                      text: |
                        <!channel> *CRITICAL ALERT*
                        
                        {{ range .Alerts }}
                        *Alert:* {{ .Labels.alertname }}
                        *Summary:* {{ .Annotations.summary }}
                        *Description:* {{ .Annotations.description }}
                        *Cluster:* ${{ env.CLUSTER_NAME }}
                        *Namespace:* {{ .Labels.namespace }}
                        {{ if .Labels.pod }}*Pod:* {{ .Labels.pod }}{{ end }}
                        *Started:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
                        {{ end }}
                      color: 'danger'
                
                - name: 'slack-warnings'
                  slack_configs:
                    - channel: '#${{ github.event.inputs.slack_channel }}'
                      send_resolved: true
                      title: 'âš ï¸ [WARNING] {{ .CommonLabels.alertname }}'
                      text: |
                        {{ range .Alerts }}
                        *Alert:* {{ .Labels.alertname }}
                        *Summary:* {{ .Annotations.summary }}
                        *Description:* {{ .Annotations.description }}
                        *Cluster:* ${{ env.CLUSTER_NAME }}
                        *Namespace:* {{ .Labels.namespace }}
                        {{ if .Labels.pod }}*Pod:* {{ .Labels.pod }}{{ end }}
                        {{ end }}
                      color: 'warning'
              
              inhibit_rules:
                - source_match:
                    severity: 'critical'
                  target_match:
                    severity: 'warning'
                  equal: ['alertname', 'cluster', 'service']
          EOF

      - name: Update Prometheus stack with new AlertManager config
        run: |
          helm upgrade kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --reuse-values \
            -f /tmp/alertmanager-values.yaml \
            --wait

      - name: Verify AlertManager configuration
        run: |
          echo "Waiting for AlertManager to reload configuration..."
          sleep 10
          
          POD=$(kubectl get pods -n monitoring -l app.kubernetes.io/name=alertmanager -o jsonpath='{.items[0].metadata.name}')
          echo "AlertManager pod: $POD"
          
          kubectl logs -n monitoring $POD -c alertmanager --tail=50 || true

      - name: Send test alert to Slack
        run: |
          cat > /tmp/test-alert.yaml <<EOF
          apiVersion: v1
          kind: Pod
          metadata:
            name: alertmanager-test-alert
            namespace: monitoring
          spec:
            containers:
            - name: curl
              image: curlimages/curl:latest
              command:
              - sh
              - -c
              - |
                curl -X POST http://kube-prometheus-stack-alertmanager.monitoring.svc.cluster.local:9093/api/v1/alerts -H 'Content-Type: application/json' -d '[
                  {
                    "labels": {
                      "alertname": "TestAlert",
                      "severity": "info",
                      "cluster": "${{ env.CLUSTER_NAME }}",
                      "namespace": "monitoring"
                    },
                    "annotations": {
                      "summary": "Test Alert from GitHub Actions",
                      "description": "This is a test alert to verify Slack integration is working correctly. If you see this message, the configuration was successful! ðŸŽ‰"
                    },
                    "startsAt": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                    "endsAt": "$(date -u -d '+5 minutes' +%Y-%m-%dT%H:%M:%SZ)"
                  }
                ]'
            restartPolicy: Never
          EOF
          
          kubectl apply -f /tmp/test-alert.yaml
          sleep 5
          kubectl logs -n monitoring alertmanager-test-alert || true
          kubectl delete pod -n monitoring alertmanager-test-alert --ignore-not-found

      - name: Display AlertManager status
        run: |
          echo "=== AlertManager Configuration Summary ==="
          echo "Slack Channel: #${{ github.event.inputs.slack_channel }}"
          echo "Minimum Severity: ${{ github.event.inputs.severity_filter }}"
          echo "Cluster: ${{ env.CLUSTER_NAME }}"
          echo ""
          echo "AlertManager is now configured to send alerts to Slack!"
          echo "Check your Slack channel for the test alert."
          echo ""
          echo "To access AlertManager UI:"
          echo "kubectl port-forward -n monitoring svc/kube-prometheus-stack-alertmanager 9093:9093"
          echo "Then open: http://localhost:9093"

      - name: Create alert rules documentation
        run: |
          cat > /tmp/active-alerts.md <<'EOF'
          # Active Prometheus Alert Rules

          ## ðŸš¨ Critical Alerts
          - **KubePodCrashLooping** - Pod is crash looping
          - **KubeNodeNotReady** - Node is not ready
          - **KubePersistentVolumeFillingUp** - PV is filling up
          - **TargetDown** - Prometheus target is down

          ## âš ï¸ Warning Alerts
          - **KubePodNotReady** - Pod has been in non-ready state for >15 minutes
          - **KubeDeploymentReplicasMismatch** - Deployment has not matched expected replicas
          - **KubeMemoryOvercommit** - Cluster has overcommitted memory
          - **KubeCPUOvercommit** - Cluster has overcommitted CPU

          ## ðŸ“Š Info Alerts
          - **Watchdog** - Always-firing alert (used for dead man's switch)

          ## Custom Alerts (can be added)
          - High response time (>500ms)
          - Database connection pool exhaustion
          - High error rate (>5%)
          - Certificate expiration (<30 days)

          ## Slack Alert Format
          - ðŸš¨ Critical: Mentions @channel
          - âš ï¸ Warning: Standard notification
          - ðŸ”” Info: Standard notification
          - âœ… Resolved: Green color indicator

          EOF
          
          cat /tmp/active-alerts.md
