name: ğŸ—‘ï¸ Destroy AWS Environment

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to destroy'
        required: true
        type: choice
        options:
          - stg
          - prd
      confirmation:
        description: 'Type "DESTROY" to confirm complete destruction'
        required: true
        type: string
      preserve_ecr_images:
        description: 'Preserve ECR images (keep Docker images)'
        required: false
        type: boolean
        default: false
      destroy_bootstrap:
        description: 'âš ï¸ ALSO DESTROY BOOTSTRAP (S3 + DynamoDB) - CANNOT BE RECOVERED! âš ï¸'
        required: false
        type: boolean
        default: false

env:
  AWS_REGION: us-east-1
  TF_VERSION: '1.6.0'

jobs:
  validate-confirmation:
    name: âœ… Validate Destruction Request
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ”’ Validate Confirmation
        run: |
          if [ "${{ github.event.inputs.confirmation }}" != "DESTROY" ]; then
            echo "âŒ ERROR: Confirmation word must be exactly 'DESTROY'"
            echo "   You entered: '${{ github.event.inputs.confirmation }}'"
            exit 1
          fi
          echo "âœ… Confirmation validated"
      
      - name: ğŸ“‹ Show Destruction Plan
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ğŸ—‘ï¸  COMPLETE ENVIRONMENT DESTRUCTION"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "Environment: ${{ github.event.inputs.environment }}"
          echo ""
          echo "Will destroy:"
          echo "  âœ“ EKS Cluster (tx01-eks-${{ github.event.inputs.environment }})"
          echo "  âœ“ RDS Database (tx01-db-${{ github.event.inputs.environment }})"
          echo "  âœ“ Application Load Balancers"
          echo "  âœ“ NAT Gateways"
          echo "  âœ“ VPC and all networking"
          echo "  âœ“ EC2 Instances"
          echo "  âœ“ EBS Volumes"
          echo "  âœ“ Security Groups"
          echo "  âœ“ IAM Roles (EKS-related)"
          if [ "${{ github.event.inputs.preserve_ecr_images }}" = "false" ]; then
            echo "  âœ“ ECR Repositories and Images"
          fi
          echo ""
          echo "Will PRESERVE:"
          echo "  âœ“ S3 Terraform State Bucket"
          echo "  âœ“ DynamoDB State Lock Table"
          if [ "${{ github.event.inputs.preserve_ecr_images }}" = "true" ]; then
            echo "  âœ“ ECR Images"
          fi
          echo ""
          echo "âš ï¸  THIS ACTION CANNOT BE UNDONE!"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

  destroy-backup-infrastructure:
    name: ğŸ—‘ï¸ Destroy AWS Backup Infrastructure
    needs: validate-confirmation
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ”§ Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: ğŸ—‘ï¸ List Backup Resources (Before)
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ğŸ“¦ AWS BACKUP RESOURCES TO BE DESTROYED"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          
          echo "Backup Vaults:"
          aws backup list-backup-vaults \
            --region ${{ env.AWS_REGION }} \
            --query 'BackupVaultList[*].[BackupVaultName,NumberOfRecoveryPoints]' \
            --output table || echo "  None found"
          
          echo ""
          echo "Backup Plans:"
          aws backup list-backup-plans \
            --region ${{ env.AWS_REGION }} \
            --query 'BackupPlansList[*].[BackupPlanName,BackupPlanId]' \
            --output table || echo "  None found"
          
          echo ""
          echo "IAM Backup Roles:"
          aws iam list-roles \
            --query 'Roles[?contains(RoleName, `backup`)].RoleName' \
            --output table || echo "  None found"
      
      - name: ğŸ—‘ï¸ Delete All Recovery Points
        continue-on-error: true
        run: |
          echo "ğŸ—‘ï¸ Deleting all recovery points from backup vault..."
          
          VAULT_NAME="tx01-backup-vault-${{ github.event.inputs.environment }}"
          
          # List all recovery points
          RECOVERY_POINTS=$(aws backup list-recovery-points-by-backup-vault \
            --backup-vault-name $VAULT_NAME \
            --region ${{ env.AWS_REGION }} \
            --query 'RecoveryPoints[].RecoveryPointArn' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$RECOVERY_POINTS" ]; then
            for RP_ARN in $RECOVERY_POINTS; do
              echo "  Deleting recovery point: $RP_ARN"
              aws backup delete-recovery-point \
                --backup-vault-name $VAULT_NAME \
                --recovery-point-arn $RP_ARN \
                --region ${{ env.AWS_REGION }} || true
            done
            
            echo "â³ Waiting for recovery points deletion (60s)..."
            sleep 60
            echo "âœ… Recovery points deleted"
          else
            echo "âœ… No recovery points found"
          fi
      
      - name: ğŸ—‘ï¸ Delete Backup Selections
        continue-on-error: true
        run: |
          echo "ğŸ—‘ï¸ Deleting backup selections..."
          
          BACKUP_PLAN_ID=$(aws backup list-backup-plans \
            --region ${{ env.AWS_REGION }} \
            --query "BackupPlansList[?BackupPlanName=='tx01-daily-backup-${{ github.event.inputs.environment }}'].BackupPlanId" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$BACKUP_PLAN_ID" ] && [ "$BACKUP_PLAN_ID" != "None" ]; then
            echo "Found backup plan: $BACKUP_PLAN_ID"
            
            # List and delete backup selections
            SELECTIONS=$(aws backup list-backup-selections \
              --backup-plan-id $BACKUP_PLAN_ID \
              --region ${{ env.AWS_REGION }} \
              --query 'BackupSelectionsList[].SelectionId' \
              --output text 2>/dev/null || echo "")
            
            if [ -n "$SELECTIONS" ]; then
              for SELECTION_ID in $SELECTIONS; do
                echo "  Deleting backup selection: $SELECTION_ID"
                aws backup delete-backup-selection \
                  --backup-plan-id $BACKUP_PLAN_ID \
                  --selection-id $SELECTION_ID \
                  --region ${{ env.AWS_REGION }} || true
              done
              echo "âœ… Backup selections deleted"
            else
              echo "âœ… No backup selections found"
            fi
          else
            echo "âœ… No backup plan found"
          fi
      
      - name: ğŸ—‘ï¸ Delete Backup Plan
        continue-on-error: true
        run: |
          echo "ğŸ—‘ï¸ Deleting backup plan..."
          
          BACKUP_PLAN_ID=$(aws backup list-backup-plans \
            --region ${{ env.AWS_REGION }} \
            --query "BackupPlansList[?BackupPlanName=='tx01-daily-backup-${{ github.event.inputs.environment }}'].BackupPlanId" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$BACKUP_PLAN_ID" ] && [ "$BACKUP_PLAN_ID" != "None" ]; then
            echo "  Deleting backup plan: $BACKUP_PLAN_ID"
            aws backup delete-backup-plan \
              --backup-plan-id $BACKUP_PLAN_ID \
              --region ${{ env.AWS_REGION }} || true
            echo "âœ… Backup plan deleted"
          else
            echo "âœ… No backup plan found"
          fi
      
      - name: ğŸ—‘ï¸ Delete Backup Vault
        continue-on-error: true
        run: |
          echo "ğŸ—‘ï¸ Deleting backup vault..."
          
          VAULT_NAME="tx01-backup-vault-${{ github.event.inputs.environment }}"
          
          if aws backup describe-backup-vault --backup-vault-name $VAULT_NAME --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "  Deleting vault: $VAULT_NAME"
            aws backup delete-backup-vault \
              --backup-vault-name $VAULT_NAME \
              --region ${{ env.AWS_REGION }} || true
            echo "âœ… Backup vault deleted"
          else
            echo "âœ… Backup vault not found"
          fi
      
      - name: ğŸ—‘ï¸ Delete IAM Backup Role
        continue-on-error: true
        run: |
          echo "ğŸ—‘ï¸ Deleting IAM backup role..."
          
          ROLE_NAME="tx01-backup-role-${{ github.event.inputs.environment }}"
          
          if aws iam get-role --role-name $ROLE_NAME 2>/dev/null; then
            echo "Found IAM role: $ROLE_NAME"
            
            # List and detach managed policies
            echo "  Detaching managed policies..."
            aws iam list-attached-role-policies \
              --role-name $ROLE_NAME \
              --query 'AttachedPolicies[].PolicyArn' \
              --output text | while read policy_arn; do
                if [ -n "$policy_arn" ]; then
                  echo "    - Detaching: $policy_arn"
                  aws iam detach-role-policy \
                    --role-name $ROLE_NAME \
                    --policy-arn $policy_arn || true
                fi
              done
            
            # List and delete inline policies
            echo "  Deleting inline policies..."
            aws iam list-role-policies \
              --role-name $ROLE_NAME \
              --query 'PolicyNames[]' \
              --output text | while read policy_name; do
                if [ -n "$policy_name" ]; then
                  echo "    - Deleting: $policy_name"
                  aws iam delete-role-policy \
                    --role-name $ROLE_NAME \
                    --policy-name $policy_name || true
                fi
              done
            
            # Delete the role
            echo "  Deleting role: $ROLE_NAME"
            aws iam delete-role --role-name $ROLE_NAME || true
            echo "âœ… IAM backup role deleted"
          else
            echo "âœ… IAM backup role not found"
          fi
      
      - name: ğŸ“Š Backup Cleanup Summary
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "âœ… AWS BACKUP INFRASTRUCTURE DESTROYED"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "Remaining backup resources:"
          aws backup list-backup-vaults \
            --region ${{ env.AWS_REGION }} \
            --query 'BackupVaultList[*].[BackupVaultName,NumberOfRecoveryPoints]' \
            --output table || echo "  âœ… No backup vaults found"

  destroy-kubernetes-resources:
    name: ğŸ—‘ï¸ Clean Kubernetes Resources
    needs: [validate-confirmation, destroy-backup-infrastructure]
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4
      
      - name: ğŸ”§ Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: â˜¸ï¸ Configure kubectl
        run: |
          echo "â˜¸ï¸ Checking if EKS cluster exists..."
          if aws eks describe-cluster --name tx01-eks-${{ github.event.inputs.environment }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âœ… Cluster exists, configuring kubectl..."
            aws eks update-kubeconfig \
              --name tx01-eks-${{ github.event.inputs.environment }} \
              --region ${{ env.AWS_REGION }}
          else
            echo "âš ï¸  EKS cluster not found, skipping kubectl configuration"
            exit 0
          fi
      
      - name: ğŸ—‘ï¸ Delete All LoadBalancer Services
        continue-on-error: true
        run: |
          echo "ğŸ—‘ï¸ Deleting all LoadBalancer services (to remove ALBs/NLBs)..."
          
          # Delete LoadBalancer services in all namespaces
          kubectl get svc -A -o json | \
            jq -r '.items[] | select(.spec.type=="LoadBalancer") | "\(.metadata.namespace) \(.metadata.name)"' | \
            while read namespace name; do
              echo "  Deleting LoadBalancer: $namespace/$name"
              kubectl delete svc -n $namespace $name --timeout=60s || true
            done
          
          echo "â³ Waiting 60s for AWS to process deletions..."
          sleep 60
          
          echo "âœ… LoadBalancer services deleted"
      
      - name: ğŸ—‘ï¸ Delete Ingress Resources
        continue-on-error: true
        run: |
          echo "ğŸ—‘ï¸ Deleting all Ingress resources..."
          kubectl delete ingress --all -A --timeout=60s || true
          sleep 30
          echo "âœ… Ingress resources deleted"
      
      - name: ğŸ—‘ï¸ Delete Persistent Volume Claims
        continue-on-error: true
        run: |
          echo "ğŸ—‘ï¸ Deleting all PVCs (will delete EBS volumes)..."
          kubectl delete pvc --all -A --timeout=120s || true
          sleep 30
          echo "âœ… PVCs deleted"
      
      - name: ğŸ—‘ï¸ Delete Monitoring Namespace
        continue-on-error: true
        run: |
          echo "ğŸ—‘ï¸ Deleting monitoring namespace..."
          kubectl delete namespace monitoring --timeout=120s || true
          echo "âœ… Monitoring namespace deleted"

  destroy-terraform-resources:
    name: ğŸ—‘ï¸ Destroy Terraform Infrastructure
    needs: destroy-kubernetes-resources
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4
      
      - name: ğŸ”§ Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: ğŸ”§ Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
      
      - name: ğŸ”„ Terraform Init
        run: |
          cd terraform/${{ github.event.inputs.environment }}
          terraform init
      
      - name: ğŸ—‘ï¸ Terraform Destroy
        run: |
          cd terraform/${{ github.event.inputs.environment }}
          
          echo "ğŸ—‘ï¸ Running terraform destroy..."
          echo ""
          
          terraform destroy -auto-approve \
            -var="environment=${{ github.event.inputs.environment }}" \
            || echo "âš ï¸  Some resources may have failed to destroy (will clean up manually)"
      
      - name: ğŸ§¹ Force Delete Auto Scaling Groups
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Force deleting Auto Scaling Groups..."
          
          # Find all ASGs with tx01 in the name
          aws autoscaling describe-auto-scaling-groups \
            --region ${{ env.AWS_REGION }} \
            --query 'AutoScalingGroups[?contains(AutoScalingGroupName, `tx01`)].AutoScalingGroupName' \
            --output text | while read asg_name; do
              if [ -n "$asg_name" ]; then
                echo "  Deleting ASG: $asg_name"
                
                # Force delete (terminates all instances)
                aws autoscaling delete-auto-scaling-group \
                  --auto-scaling-group-name "$asg_name" \
                  --force-delete \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          echo "â³ Waiting 60s for ASG instances to terminate..."
          sleep 60
      
      - name: ğŸ§¹ Clean Orphaned EKS Resources
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Cleaning orphaned EKS node groups..."
          
          CLUSTER_NAME="tx01-eks-${{ github.event.inputs.environment }}"
          
          # Check if cluster still exists
          if aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "Cluster found, proceeding with deletion..."
            
            # Delete node groups first
            echo "Deleting node groups..."
            aws eks list-nodegroups \
              --cluster-name $CLUSTER_NAME \
              --region ${{ env.AWS_REGION }} \
              --query 'nodegroups[]' \
              --output text | while read nodegroup; do
                if [ -n "$nodegroup" ]; then
                  echo "  Deleting nodegroup: $nodegroup"
                  aws eks delete-nodegroup \
                    --cluster-name $CLUSTER_NAME \
                    --nodegroup-name $nodegroup \
                    --region ${{ env.AWS_REGION }} || true
                fi
              done
            
            echo "â³ Waiting for nodegroups to be deleted..."
            for i in {1..30}; do
              NODEGROUPS=$(aws eks list-nodegroups \
                --cluster-name $CLUSTER_NAME \
                --region ${{ env.AWS_REGION }} \
                --query 'nodegroups[]' \
                --output text 2>/dev/null || echo "")
              
              if [ -z "$NODEGROUPS" ]; then
                echo "âœ… All nodegroups deleted"
                break
              fi
              
              echo "  Still waiting... ($i/30)"
              sleep 30
            done
            
            # Delete addons
            echo "Deleting EKS addons..."
            aws eks list-addons \
              --cluster-name $CLUSTER_NAME \
              --region ${{ env.AWS_REGION }} \
              --query 'addons[]' \
              --output text | while read addon; do
                if [ -n "$addon" ]; then
                  echo "  Deleting addon: $addon"
                  aws eks delete-addon \
                    --cluster-name $CLUSTER_NAME \
                    --addon-name $addon \
                    --region ${{ env.AWS_REGION }} || true
                fi
              done
            
            sleep 60
            
            # Delete cluster
            echo "Deleting EKS cluster..."
            aws eks delete-cluster \
              --name $CLUSTER_NAME \
              --region ${{ env.AWS_REGION }} || true
            
            echo "â³ Waiting for cluster deletion (max 15 min)..."
            for i in {1..30}; do
              if ! aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} 2>/dev/null; then
                echo "âœ… Cluster deleted successfully"
                break
              fi
              echo "  Still waiting... ($i/30)"
              sleep 30
            done
          else
            echo "âœ… EKS cluster already deleted"
          fi
      
      - name: ğŸ§¹ Clean Orphaned RDS Instances
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Cleaning orphaned RDS instances..."
          
          # Find RDS instances with tx01 prefix
          RDS_INSTANCES=$(aws rds describe-db-instances \
            --region ${{ env.AWS_REGION }} \
            --query "DBInstances[?contains(DBInstanceIdentifier, 'tx01-') && contains(DBInstanceIdentifier, '-${{ github.event.inputs.environment }}')].DBInstanceIdentifier" \
            --output text)
          
          if [ -n "$RDS_INSTANCES" ]; then
            for DB_NAME in $RDS_INSTANCES; do
              echo "Deleting RDS instance: $DB_NAME"
              
              # Delete without final snapshot (destructive!)
              aws rds delete-db-instance \
                --db-instance-identifier $DB_NAME \
                --skip-final-snapshot \
                --delete-automated-backups \
                --region ${{ env.AWS_REGION }} || true
            done
            
            echo "â³ Waiting for RDS deletion..."
            sleep 60
          else
            echo "âœ… No RDS instances found to delete"
          fi
      
      - name: ğŸ§¹ Clean Orphaned Load Balancers
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Cleaning orphaned load balancers..."
          
          # Find ALBs with tx01 or k8s tags
          aws elbv2 describe-load-balancers \
            --region ${{ env.AWS_REGION }} \
            --query 'LoadBalancers[?contains(LoadBalancerName, `tx01`) || contains(LoadBalancerName, `k8s`)].LoadBalancerArn' \
            --output text | while read alb_arn; do
              if [ -n "$alb_arn" ]; then
                echo "  Deleting ALB: $alb_arn"
                aws elbv2 delete-load-balancer \
                  --load-balancer-arn $alb_arn \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          sleep 30
          echo "âœ… Load balancers cleaned"
      
      - name: ğŸ§¹ Clean Orphaned Target Groups
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Cleaning orphaned target groups..."
          
          aws elbv2 describe-target-groups \
            --region ${{ env.AWS_REGION }} \
            --query 'TargetGroups[?contains(TargetGroupName, `tx01`) || contains(TargetGroupName, `k8s`)].TargetGroupArn' \
            --output text | while read tg_arn; do
              if [ -n "$tg_arn" ]; then
                echo "  Deleting Target Group: $tg_arn"
                aws elbv2 delete-target-group \
                  --target-group-arn $tg_arn \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          echo "âœ… Target groups cleaned"
      
      - name: ğŸ§¹ Terminate All EC2 Instances
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Terminating all EC2 instances..."
          
          # Find all instances with tx01 or eks tags
          INSTANCE_IDS=$(aws ec2 describe-instances \
            --filters "Name=tag:eks:cluster-name,Values=tx01-eks-${{ github.event.inputs.environment }}" \
            --query 'Reservations[].Instances[?State.Name!=`terminated`].InstanceId' \
            --output text \
            --region ${{ env.AWS_REGION }})
          
          if [ -n "$INSTANCE_IDS" ]; then
            echo "  Terminating instances: $INSTANCE_IDS"
            aws ec2 terminate-instances \
              --instance-ids $INSTANCE_IDS \
              --region ${{ env.AWS_REGION }} || true
            
            echo "â³ Waiting for instances to terminate..."
            sleep 120
          else
            echo "âœ… No instances to terminate"
          fi
      
      - name: ğŸ§¹ Delete Launch Templates
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Deleting launch templates..."
          
          aws ec2 describe-launch-templates \
            --filters "Name=tag:Project,Values=tx01" \
            --query 'LaunchTemplates[].LaunchTemplateId' \
            --output text \
            --region ${{ env.AWS_REGION }} | while read lt_id; do
              if [ -n "$lt_id" ]; then
                echo "  Deleting launch template: $lt_id"
                aws ec2 delete-launch-template \
                  --launch-template-id $lt_id \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
      
      - name: ğŸ§¹ Delete Network Interfaces
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Deleting orphaned network interfaces..."
          
          # Wait for instances to fully terminate
          sleep 60
          
          # Find available ENIs
          aws ec2 describe-network-interfaces \
            --filters "Name=status,Values=available" "Name=tag:cluster.k8s.amazonaws.com/name,Values=tx01-eks-${{ github.event.inputs.environment }}" \
            --query 'NetworkInterfaces[].NetworkInterfaceId' \
            --output text \
            --region ${{ env.AWS_REGION }} | while read eni_id; do
              if [ -n "$eni_id" ]; then
                echo "  Deleting ENI: $eni_id"
                aws ec2 delete-network-interface \
                  --network-interface-id $eni_id \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          sleep 30
      
      - name: ğŸ§¹ Clean Orphaned Security Groups
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Cleaning orphaned security groups..."
          
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=tx01-vpc-${{ github.event.inputs.environment }}" \
            --query 'Vpcs[0].VpcId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
          
          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            echo "Found VPC: $VPC_ID"
            
            # Multiple passes to handle dependencies
            for pass in {1..3}; do
              echo "  Pass $pass/3..."
              
              # Remove all security group rules first
              aws ec2 describe-security-groups \
                --filters "Name=vpc-id,Values=$VPC_ID" \
                --query 'SecurityGroups[?GroupName!=`default`].GroupId' \
                --output text \
                --region ${{ env.AWS_REGION }} | while read sg_id; do
                  if [ -n "$sg_id" ]; then
                    echo "    Revoking rules for SG: $sg_id"
                    
                    # Revoke ingress rules
                    aws ec2 describe-security-groups \
                      --group-ids $sg_id \
                      --region ${{ env.AWS_REGION }} \
                      --query 'SecurityGroups[0].IpPermissions' \
                      --output json > /tmp/ingress_$sg_id.json 2>/dev/null || true
                    
                    if [ -s /tmp/ingress_$sg_id.json ] && [ "$(cat /tmp/ingress_$sg_id.json)" != "[]" ] && [ "$(cat /tmp/ingress_$sg_id.json)" != "null" ]; then
                      aws ec2 revoke-security-group-ingress \
                        --group-id $sg_id \
                        --ip-permissions file:///tmp/ingress_$sg_id.json \
                        --region ${{ env.AWS_REGION }} 2>/dev/null || true
                    fi
                    
                    # Revoke egress rules
                    aws ec2 describe-security-groups \
                      --group-ids $sg_id \
                      --region ${{ env.AWS_REGION }} \
                      --query 'SecurityGroups[0].IpPermissionsEgress' \
                      --output json > /tmp/egress_$sg_id.json 2>/dev/null || true
                    
                    if [ -s /tmp/egress_$sg_id.json ] && [ "$(cat /tmp/egress_$sg_id.json)" != "[]" ] && [ "$(cat /tmp/egress_$sg_id.json)" != "null" ]; then
                      aws ec2 revoke-security-group-egress \
                        --group-id $sg_id \
                        --ip-permissions file:///tmp/egress_$sg_id.json \
                        --region ${{ env.AWS_REGION }} 2>/dev/null || true
                    fi
                  fi
                done
              
              sleep 10
              
              # Delete security groups
              aws ec2 describe-security-groups \
                --filters "Name=vpc-id,Values=$VPC_ID" \
                --query 'SecurityGroups[?GroupName!=`default`].GroupId' \
                --output text \
                --region ${{ env.AWS_REGION }} | while read sg_id; do
                  if [ -n "$sg_id" ]; then
                    echo "    Deleting SG: $sg_id"
                    aws ec2 delete-security-group \
                      --group-id $sg_id \
                      --region ${{ env.AWS_REGION }} 2>/dev/null || true
                  fi
                done
              
              sleep 10
            done
          fi
          
          echo "âœ… Security groups cleaned"
      
      - name: ğŸ§¹ Clean Orphaned NAT Gateways
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Cleaning orphaned NAT gateways..."
          
          aws ec2 describe-nat-gateways \
            --filter "Name=tag:Name,Values=*tx01*" \
            --query 'NatGateways[?State==`available`].NatGatewayId' \
            --output text \
            --region ${{ env.AWS_REGION }} | while read nat_id; do
              if [ -n "$nat_id" ]; then
                echo "  Deleting NAT Gateway: $nat_id"
                aws ec2 delete-nat-gateway \
                  --nat-gateway-id $nat_id \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          echo "â³ Waiting for NAT gateways deletion..."
          sleep 60
          
          echo "âœ… NAT gateways cleaned"
      
      - name: ğŸ§¹ Release Elastic IPs
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Releasing Elastic IPs..."
          
          aws ec2 describe-addresses \
            --filters "Name=tag:Name,Values=*tx01*" \
            --query 'Addresses[].AllocationId' \
            --output text \
            --region ${{ env.AWS_REGION }} | while read eip_id; do
              if [ -n "$eip_id" ]; then
                echo "  Releasing EIP: $eip_id"
                aws ec2 release-address \
                  --allocation-id $eip_id \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          echo "âœ… Elastic IPs released"
      
      - name: ğŸ§¹ Clean Orphaned EBS Volumes
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Cleaning orphaned EBS volumes..."
          
          aws ec2 describe-volumes \
            --filters "Name=tag:Name,Values=*tx01*" "Name=status,Values=available" \
            --query 'Volumes[].VolumeId' \
            --output text \
            --region ${{ env.AWS_REGION }} | while read vol_id; do
              if [ -n "$vol_id" ]; then
                echo "  Deleting volume: $vol_id"
                aws ec2 delete-volume \
                  --volume-id $vol_id \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          echo "âœ… EBS volumes cleaned"
      
      - name: ğŸ§¹ Delete VPC
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Deleting VPC..."
          
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=tx01-vpc-${{ github.event.inputs.environment }}" \
            --query 'Vpcs[0].VpcId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
          
          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            echo "Deleting VPC: $VPC_ID"
            
            # Delete subnets
            echo "  Deleting subnets..."
            aws ec2 describe-subnets \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query 'Subnets[].SubnetId' \
              --output text \
              --region ${{ env.AWS_REGION }} | while read subnet_id; do
                if [ -n "$subnet_id" ]; then
                  echo "    Subnet: $subnet_id"
                  aws ec2 delete-subnet \
                    --subnet-id $subnet_id \
                    --region ${{ env.AWS_REGION }} || true
                fi
              done
            
            sleep 10
            
            # Delete route tables
            echo "  Deleting route tables..."
            aws ec2 describe-route-tables \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' \
              --output text \
              --region ${{ env.AWS_REGION }} | while read rt_id; do
                if [ -n "$rt_id" ]; then
                  echo "    Route table: $rt_id"
                  aws ec2 delete-route-table \
                    --route-table-id $rt_id \
                    --region ${{ env.AWS_REGION }} || true
                fi
              done
            
            sleep 10
            
            # Delete internet gateway
            echo "  Deleting internet gateways..."
            aws ec2 describe-internet-gateways \
              --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
              --query 'InternetGateways[].InternetGatewayId' \
              --output text \
              --region ${{ env.AWS_REGION }} | while read igw_id; do
                if [ -n "$igw_id" ]; then
                  echo "    IGW: $igw_id"
                  aws ec2 detach-internet-gateway \
                    --internet-gateway-id $igw_id \
                    --vpc-id $VPC_ID \
                    --region ${{ env.AWS_REGION }} || true
                  aws ec2 delete-internet-gateway \
                    --internet-gateway-id $igw_id \
                    --region ${{ env.AWS_REGION }} || true
                fi
              done
            
            sleep 10
            
            # Finally delete VPC
            echo "  Deleting VPC: $VPC_ID"
            aws ec2 delete-vpc \
              --vpc-id $VPC_ID \
              --region ${{ env.AWS_REGION }} || true
          fi
          
          echo "âœ… VPC cleaned"
      
      - name: ğŸ§¹ Delete CloudWatch Log Groups
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Deleting CloudWatch log groups..."
          
          aws logs describe-log-groups \
            --log-group-name-prefix "/aws/rds/instance/tx01-db-${{ github.event.inputs.environment }}" \
            --region ${{ env.AWS_REGION }} \
            --query 'logGroups[].logGroupName' \
            --output text | while read log_group; do
              if [ -n "$log_group" ]; then
                echo "  Deleting log group: $log_group"
                aws logs delete-log-group \
                  --log-group-name "$log_group" \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          aws logs describe-log-groups \
            --log-group-name-prefix "/aws/eks/tx01-eks-${{ github.event.inputs.environment }}" \
            --region ${{ env.AWS_REGION }} \
            --query 'logGroups[].logGroupName' \
            --output text | while read log_group; do
              if [ -n "$log_group" ]; then
                echo "  Deleting log group: $log_group"
                aws logs delete-log-group \
                  --log-group-name "$log_group" \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          # Delete EC2 log groups
          aws logs describe-log-groups \
            --log-group-name-prefix "/aws/ec2/tx01-${{ github.event.inputs.environment }}" \
            --region ${{ env.AWS_REGION }} \
            --query 'logGroups[].logGroupName' \
            --output text | while read log_group; do
              if [ -n "$log_group" ]; then
                echo "  Deleting log group: $log_group"
                aws logs delete-log-group \
                  --log-group-name "$log_group" \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          # Delete WAF log groups
          aws logs describe-log-groups \
            --log-group-name-prefix "/aws/waf/tx01-${{ github.event.inputs.environment }}" \
            --region ${{ env.AWS_REGION }} \
            --query 'logGroups[].logGroupName' \
            --output text | while read log_group; do
              if [ -n "$log_group" ]; then
                echo "  Deleting log group: $log_group"
                aws logs delete-log-group \
                  --log-group-name "$log_group" \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
      
      - name: ğŸ§¹ Delete Secrets Manager Secrets
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Deleting Secrets Manager secrets..."
          
          aws secretsmanager list-secrets \
            --region ${{ env.AWS_REGION }} \
            --query 'SecretList[?contains(Name, `tx01`)].Name' \
            --output text | while read secret_name; do
              if [ -n "$secret_name" ]; then
                echo "  Deleting secret: $secret_name"
                aws secretsmanager delete-secret \
                  --secret-id "$secret_name" \
                  --force-delete-without-recovery \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
      
      - name: ğŸ§¹ Delete RDS Snapshots
        continue-on-error: true
        run: |
          echo "ğŸ§¹ Deleting RDS snapshots..."
          
          aws rds describe-db-snapshots \
            --query 'DBSnapshots[?contains(DBInstanceIdentifier, `tx01-db-${{ github.event.inputs.environment }}`)].DBSnapshotIdentifier' \
            --region ${{ env.AWS_REGION }} \
            --output text | while read snapshot_id; do
              if [ -n "$snapshot_id" ]; then
                echo "  Deleting snapshot: $snapshot_id"
                aws rds delete-db-snapshot \
                  --db-snapshot-identifier "$snapshot_id" \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done

  cleanup-ecr:
    name: ğŸ—‘ï¸ Clean ECR Repositories
    needs: destroy-terraform-resources
    runs-on: ubuntu-latest
    if: github.event.inputs.preserve_ecr_images == 'false'
    steps:
      - name: ğŸ”§ Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: ğŸ—‘ï¸ Delete ECR Repositories
        run: |
          echo "ğŸ—‘ï¸ Deleting ECR repositories..."
          
          aws ecr describe-repositories \
            --region ${{ env.AWS_REGION }} \
            --query 'repositories[?contains(repositoryName, `tx01`) || contains(repositoryName, `dx01`)].repositoryName' \
            --output text | while read repo_name; do
              if [ -n "$repo_name" ]; then
                echo "  Deleting repository: $repo_name"
                aws ecr delete-repository \
                  --repository-name $repo_name \
                  --force \
                  --region ${{ env.AWS_REGION }} || true
              fi
            done
          
          echo "âœ… ECR repositories deleted"

  cleanup-iam:
    name: ğŸ—‘ï¸ Clean IAM Resources
    needs: destroy-terraform-resources
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ”§ Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: ğŸ—‘ï¸ Delete EKS-related IAM Roles
        continue-on-error: true
        run: |
          echo "ğŸ—‘ï¸ Cleaning EKS-related IAM roles..."
          
          # List of IAM roles to delete
          ROLES=(
            "tx01-eks-cluster-role-${{ github.event.inputs.environment }}"
            "tx01-eks-node-role-${{ github.event.inputs.environment }}"
            "tx01-eks-ebs-csi-driver"
            "tx01-alb-controller-${{ github.event.inputs.environment }}"
          )
          
          for role in "${ROLES[@]}"; do
            if aws iam get-role --role-name $role 2>/dev/null; then
              echo "  Deleting role: $role"
              
              # Detach all policies
              aws iam list-attached-role-policies \
                --role-name $role \
                --query 'AttachedPolicies[].PolicyArn' \
                --output text | while read policy_arn; do
                  echo "    Detaching policy: $policy_arn"
                  aws iam detach-role-policy \
                    --role-name $role \
                    --policy-arn $policy_arn || true
                done
              
              # Delete inline policies
              aws iam list-role-policies \
                --role-name $role \
                --query 'PolicyNames[]' \
                --output text | while read policy_name; do
                  echo "    Deleting inline policy: $policy_name"
                  aws iam delete-role-policy \
                    --role-name $role \
                    --policy-name $policy_name || true
                done
              
              # Delete role
              aws iam delete-role --role-name $role || true
            fi
          done
          
          echo "âœ… IAM roles cleaned"

  destroy-bootstrap:
    name: âš ï¸ Destroy Terraform Bootstrap (S3 + DynamoDB)
    needs: [destroy-terraform-resources, cleanup-ecr, cleanup-iam]
    if: github.event.inputs.destroy_bootstrap == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ”§ Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: âš ï¸ Final Warning
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "âš ï¸  FINAL WARNING - BOOTSTRAP DESTRUCTION âš ï¸"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "ğŸ”¥ About to destroy Terraform backend infrastructure:"
          echo ""
          echo "   ğŸ“¦ S3 Bucket: tx01-terraform-state-maringelix-2025"
          echo "   ğŸ”’ DynamoDB Table: tx01-terraform-state-maringelix-2025-locks"
          echo ""
          echo "âš ï¸  THIS ACTION CANNOT BE UNDONE!"
          echo "âš ï¸  All Terraform state will be PERMANENTLY LOST!"
          echo "âš ï¸  You will need to run terraform-bootstrap again to recreate!"
          echo ""
          echo "â³ Proceeding in 10 seconds..."
          sleep 10
      
      - name: ğŸ—‘ï¸ Empty S3 Bucket
        run: |
          echo "ğŸ—‘ï¸ Emptying S3 bucket..."
          
          BUCKET_NAME="tx01-terraform-state-maringelix-2025"
          
          if aws s3api head-bucket --bucket $BUCKET_NAME 2>/dev/null; then
            echo "Found bucket: $BUCKET_NAME"
            
            # Check if bucket has versioning
            VERSIONING=$(aws s3api get-bucket-versioning --bucket $BUCKET_NAME --query 'Status' --output text 2>/dev/null || echo "")
            
            if [ "$VERSIONING" = "Enabled" ]; then
              echo "  âš ï¸  Bucket has versioning enabled, deleting all versions..."
              
              # Delete all versions and delete markers
              aws s3api list-object-versions \
                --bucket $BUCKET_NAME \
                --output json | \
              jq -r '.Versions[]?, .DeleteMarkers[]? | "\(.Key) \(.VersionId)"' | \
              while read key version_id; do
                if [ -n "$key" ] && [ -n "$version_id" ]; then
                  echo "    - Deleting: $key (version: $version_id)"
                  aws s3api delete-object \
                    --bucket $BUCKET_NAME \
                    --key "$key" \
                    --version-id "$version_id" || true
                fi
              done
            else
              echo "  Deleting all objects..."
              aws s3 rm s3://$BUCKET_NAME --recursive || true
            fi
            
            echo "âœ… S3 bucket emptied"
          else
            echo "âš ï¸  S3 bucket not found or already deleted"
          fi
      
      - name: ğŸ—‘ï¸ Delete S3 Bucket
        run: |
          echo "ğŸ—‘ï¸ Deleting S3 bucket..."
          
          BUCKET_NAME="tx01-terraform-state-maringelix-2025"
          
          if aws s3api head-bucket --bucket $BUCKET_NAME 2>/dev/null; then
            echo "  Deleting bucket: $BUCKET_NAME"
            aws s3api delete-bucket \
              --bucket $BUCKET_NAME \
              --region ${{ env.AWS_REGION }} || true
            echo "âœ… S3 bucket deleted"
          else
            echo "âœ… S3 bucket already deleted"
          fi
      
      - name: ğŸ—‘ï¸ Delete DynamoDB Table
        run: |
          echo "ğŸ—‘ï¸ Deleting DynamoDB table..."
          
          TABLE_NAME="tx01-terraform-state-maringelix-2025-locks"
          
          if aws dynamodb describe-table --table-name $TABLE_NAME --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "  Deleting table: $TABLE_NAME"
            aws dynamodb delete-table \
              --table-name $TABLE_NAME \
              --region ${{ env.AWS_REGION }} || true
            
            echo "â³ Waiting for table deletion..."
            aws dynamodb wait table-not-exists \
              --table-name $TABLE_NAME \
              --region ${{ env.AWS_REGION }} || true
            
            echo "âœ… DynamoDB table deleted"
          else
            echo "âœ… DynamoDB table already deleted"
          fi
      
      - name: ğŸ“Š Bootstrap Destruction Summary
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "âœ… TERRAFORM BOOTSTRAP DESTROYED"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "ğŸ” Verifying deletion..."
          echo ""
          
          echo "S3 Buckets with 'tx01':"
          aws s3 ls | grep tx01 || echo "  âœ… None found"
          
          echo ""
          echo "DynamoDB Tables with 'tx01':"
          aws dynamodb list-tables \
            --query 'TableNames[?contains(@, `tx01`)]' \
            --output table || echo "  âœ… None found"
          
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "âš ï¸  ALL TERRAFORM STATE HAS BEEN DESTROYED"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "â„¹ï¸  To recreate the infrastructure, you must:"
          echo "   1. Run: .github/workflows/terraform-bootstrap.yml"
          echo "   2. Then run: .github/workflows/terraform-apply.yml"

  final-report:
    name: ğŸ“‹ Destruction Report
    needs: [destroy-kubernetes-resources, destroy-terraform-resources, cleanup-ecr, cleanup-iam, destroy-bootstrap]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ”§ Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: ğŸ“‹ Generate Destruction Report
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ğŸ“‹ DESTRUCTION REPORT - ${{ github.event.inputs.environment }}"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          
          echo "ğŸ” Checking remaining resources..."
          echo ""
          
          echo "VPCs:"
          aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*tx01*" \
            --query 'Vpcs[].{ID:VpcId,Name:Tags[?Key==`Name`]|[0].Value}' \
            --output table || echo "  âœ… None found"
          
          echo ""
          echo "EKS Clusters:"
          aws eks list-clusters \
            --query 'clusters[?contains(@, `tx01`)]' \
            --output table || echo "  âœ… None found"
          
          echo ""
          echo "RDS Instances:"
          aws rds describe-db-instances \
            --query 'DBInstances[?contains(DBInstanceIdentifier, `tx01`)].DBInstanceIdentifier' \
            --output table || echo "  âœ… None found"
          
          echo ""
          echo "Load Balancers:"
          aws elbv2 describe-load-balancers \
            --query 'LoadBalancers[?contains(LoadBalancerName, `tx01`) || contains(LoadBalancerName, `k8s`)].LoadBalancerName' \
            --output table || echo "  âœ… None found"
          
          echo ""
          echo "NAT Gateways:"
          aws ec2 describe-nat-gateways \
            --filter "Name=tag:Name,Values=*tx01*" \
            --query 'NatGateways[?State!=`deleted`].{ID:NatGatewayId,State:State}' \
            --output table || echo "  âœ… None found"
          
          if [ "${{ github.event.inputs.preserve_ecr_images }}" = "false" ]; then
            echo ""
            echo "ECR Repositories:"
            aws ecr describe-repositories \
              --query 'repositories[?contains(repositoryName, `tx01`) || contains(repositoryName, `dx01`)].repositoryName' \
              --output table || echo "  âœ… None found"
          fi
          
          echo ""
          echo "AWS Backup Vaults:"
          aws backup list-backup-vaults \
            --region ${{ env.AWS_REGION }} \
            --query 'BackupVaultList[*].[BackupVaultName,NumberOfRecoveryPoints]' \
            --output table || echo "  âœ… None found"
          
          echo ""
          if [ "${{ github.event.inputs.destroy_bootstrap }}" = "true" ]; then
            echo "ğŸ”¥ BOOTSTRAP STATUS: DESTROYED"
            echo ""
            echo "S3 Buckets with 'tx01':"
            aws s3 ls | grep tx01 || echo "  âœ… None found"
            echo ""
            echo "DynamoDB Tables with 'tx01':"
            aws dynamodb list-tables \
              --query 'TableNames[?contains(@, `tx01`)]' \
              --output table || echo "  âœ… None found"
          else
            echo "âœ… PRESERVED Resources (Bootstrap):"
            echo ""
            echo "S3 Buckets:"
            aws s3 ls | grep tx01 || echo "  None found"
            echo ""
            echo "DynamoDB Tables:"
            aws dynamodb list-tables \
              --query 'TableNames[?contains(@, `tx01`)]' \
              --output table || echo "  None found"
          fi
          
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          if [ "${{ github.event.inputs.destroy_bootstrap }}" = "true" ]; then
            echo "ğŸ”¥ COMPLETE DESTRUCTION FINISHED!"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo ""
            echo "ğŸ’° AWS Credits Status:"
            echo "   All cost-generating resources have been destroyed"
            echo "   Your Free Tier credits are now preserved"
            echo ""
            echo "âš ï¸  To recreate infrastructure:"
            echo "   1. Run terraform-bootstrap.yml workflow"
            echo "   2. Run terraform-apply.yml workflow"
          else
            echo "âœ… Environment destruction completed!"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo ""
            echo "â„¹ï¸  Bootstrap resources (S3 + DynamoDB) preserved"
            echo "   To destroy bootstrap, rerun with destroy_bootstrap=true"
          fi
