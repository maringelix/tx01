name: üîÑ Switch Environment

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment'
        required: true
        type: choice
        options:
          - stg
          - prd
        default: 'stg'
      mode:
        description: 'Compute mode'
        required: true
        type: choice
        options:
          - ec2
          - eks
          - both
        default: 'ec2'

env:
  AWS_REGION: us-east-1

jobs:
  switch:
    name: Switch to ${{ inputs.mode }} mode
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Get ALB and Target Groups info
      id: alb-info
      run: |
        # Get ALB ARN
        ALB_ARN=$(aws elbv2 describe-load-balancers \
          --region ${{ env.AWS_REGION }} \
          --query "LoadBalancers[?contains(LoadBalancerName, 'tx01-alb-${{ inputs.environment }}')].LoadBalancerArn" \
          --output text)
        echo "alb_arn=$ALB_ARN" >> $GITHUB_OUTPUT
        
        # Get Listener ARN
        LISTENER_ARN=$(aws elbv2 describe-listeners \
          --load-balancer-arn $ALB_ARN \
          --region ${{ env.AWS_REGION }} \
          --query 'Listeners[0].ListenerArn' \
          --output text)
        echo "listener_arn=$LISTENER_ARN" >> $GITHUB_OUTPUT
        
        # Get EC2 Target Group ARN
        EC2_TG_ARN=$(aws elbv2 describe-target-groups \
          --region ${{ env.AWS_REGION }} \
          --query "TargetGroups[?contains(TargetGroupName, 'tx01-tg-${{ inputs.environment }}')].TargetGroupArn" \
          --output text)
        echo "ec2_tg_arn=$EC2_TG_ARN" >> $GITHUB_OUTPUT
        
        # Try to get EKS Target Group ARN (pode n√£o existir ainda)
        EKS_TG_ARN=$(aws elbv2 describe-target-groups \
          --region ${{ env.AWS_REGION }} \
          --query "TargetGroups[?contains(TargetGroupName, 'k8s-default-tx01serv')].TargetGroupArn" \
          --output text || echo "")
        echo "eks_tg_arn=$EKS_TG_ARN" >> $GITHUB_OUTPUT

    - name: Get EC2 instances
      id: ec2-info
      run: |
        INSTANCE_IDS=$(aws ec2 describe-instances \
          --region ${{ env.AWS_REGION }} \
          --filters "Name=tag:Name,Values=tx01-ec2-*-${{ inputs.environment }}" "Name=instance-state-name,Values=running,stopped" \
          --query 'Reservations[*].Instances[*].InstanceId' \
          --output text)
        echo "instance_ids=$INSTANCE_IDS" >> $GITHUB_OUTPUT

    - name: Install kubectl (for EKS operations)
      if: inputs.mode == 'eks' || inputs.mode == 'both'
      run: |
        curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/

    - name: Configure mode - EC2 only
      if: inputs.mode == 'ec2'
      run: |
        echo "üîÑ Switching to EC2 mode..."
        
        # Deregister any pod IPs from target group
        CLUSTER_NAME=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?contains(@, 'tx01-eks-${{ inputs.environment }}')]" --output text || echo "")
        if [ -n "$CLUSTER_NAME" ]; then
          echo "üìâ Scaling down EKS deployment..."
          aws eks update-kubeconfig --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
          
          # Get pod IPs before scaling down
          POD_IPS=$(kubectl get pods -l app=tx01 -o jsonpath='{.items[*].status.podIP}' 2>/dev/null || echo "")
          
          if [ -n "$POD_IPS" ]; then
            echo "üîß Deregistering pods from target group..."
            for IP in $POD_IPS; do
              aws elbv2 deregister-targets \
                --target-group-arn ${{ steps.alb-info.outputs.ec2_tg_arn }} \
                --targets Id=$IP,Port=80 \
                --region ${{ env.AWS_REGION }} || true
            done
          fi
          
          kubectl scale deployment tx01-app --replicas=0 || true
        fi
        
        # Start EC2 instances and register in target group
        if [ -n "${{ steps.ec2-info.outputs.instance_ids }}" ]; then
          aws ec2 start-instances \
            --instance-ids ${{ steps.ec2-info.outputs.instance_ids }} \
            --region ${{ env.AWS_REGION }}
          
          echo "‚è≥ Waiting for instances to be running..."
          aws ec2 wait instance-running \
            --instance-ids ${{ steps.ec2-info.outputs.instance_ids }} \
            --region ${{ env.AWS_REGION }}
          
          echo "üîß Registering EC2 instances in target group..."
          for INSTANCE in ${{ steps.ec2-info.outputs.instance_ids }}; do
            aws elbv2 register-targets \
              --target-group-arn ${{ steps.alb-info.outputs.ec2_tg_arn }} \
              --targets Id=$INSTANCE \
              --region ${{ env.AWS_REGION }}
          done
          
          echo "‚úÖ EC2 instances started and registered"
        fi
        
        echo "‚úÖ Switched to EC2 mode"

    - name: Configure mode - EKS only
      if: inputs.mode == 'eks'
      run: |
        echo "üîÑ Switching to EKS mode..."
        
        # Get EKS cluster
        CLUSTER_NAME=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?contains(@, 'tx01-eks-${{ inputs.environment }}')]" --output text)
        
        if [ -z "$CLUSTER_NAME" ]; then
          echo "‚ùå EKS cluster not found. Please provision it first using EKS Deploy workflow."
          exit 1
        fi
        
        # Scale up EKS deployment
        echo "üìà Scaling up EKS deployment..."
        aws eks update-kubeconfig --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
        kubectl scale deployment tx01-app --replicas=2
        
        echo "‚è≥ Waiting for pods to be ready..."
        kubectl wait --for=condition=ready pod -l app=tx01 --timeout=300s
        
        # Get pod IPs
        POD_IPS=$(kubectl get pods -l app=tx01 -o jsonpath='{.items[*].status.podIP}')
        echo "Pod IPs: $POD_IPS"
        
        # Register pod IPs in target group and deregister EC2s
        echo "üîß Registering pods in target group..."
        for IP in $POD_IPS; do
          aws elbv2 register-targets \
            --target-group-arn ${{ steps.alb-info.outputs.ec2_tg_arn }} \
            --targets Id=$IP,Port=80 \
            --region ${{ env.AWS_REGION }}
        done
        
        # Deregister EC2 instances from target group
        if [ -n "${{ steps.ec2-info.outputs.instance_ids }}" ]; then
          echo "üîß Deregistering EC2 instances from target group..."
          for INSTANCE in ${{ steps.ec2-info.outputs.instance_ids }}; do
            aws elbv2 deregister-targets \
              --target-group-arn ${{ steps.alb-info.outputs.ec2_tg_arn }} \
              --targets Id=$INSTANCE \
              --region ${{ env.AWS_REGION }} || true
          done
          
          echo "üõë Stopping EC2 instances..."
          aws ec2 stop-instances \
            --instance-ids ${{ steps.ec2-info.outputs.instance_ids }} \
            --region ${{ env.AWS_REGION }}
          
          echo "‚úÖ EC2 instances stopped"
        fi
        
        echo "‚úÖ Switched to EKS mode"

    - name: Configure mode - Both EC2 and EKS
      if: inputs.mode == 'both'
      run: |
        echo "üîÑ Enabling both EC2 and EKS..."
        
        # Start EC2 instances and register in target group
        if [ -n "${{ steps.ec2-info.outputs.instance_ids }}" ]; then
          aws ec2 start-instances \
            --instance-ids ${{ steps.ec2-info.outputs.instance_ids }} \
            --region ${{ env.AWS_REGION }}
          
          echo "‚è≥ Waiting for instances to be running..."
          aws ec2 wait instance-running \
            --instance-ids ${{ steps.ec2-info.outputs.instance_ids }} \
            --region ${{ env.AWS_REGION }}
          
          echo "üîß Registering EC2 instances in target group..."
          for INSTANCE in ${{ steps.ec2-info.outputs.instance_ids }}; do
            aws elbv2 register-targets \
              --target-group-arn ${{ steps.alb-info.outputs.ec2_tg_arn }} \
              --targets Id=$INSTANCE \
              --region ${{ env.AWS_REGION }} || true
          done
          
          echo "‚úÖ EC2 instances started and registered"
        fi
        
        # Scale up EKS and register pods
        CLUSTER_NAME=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?contains(@, 'tx01-eks-${{ inputs.environment }}')]" --output text || echo "")
        if [ -n "$CLUSTER_NAME" ]; then
          echo "üìà Scaling up EKS deployment..."
          aws eks update-kubeconfig --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
          kubectl scale deployment tx01-app --replicas=2
          
          echo "‚è≥ Waiting for pods to be ready..."
          kubectl wait --for=condition=ready pod -l app=tx01 --timeout=300s
          
          # Get pod IPs and register
          POD_IPS=$(kubectl get pods -l app=tx01 -o jsonpath='{.items[*].status.podIP}')
          echo "Pod IPs: $POD_IPS"
          
          echo "üîß Registering pods in target group..."
          for IP in $POD_IPS; do
            aws elbv2 register-targets \
              --target-group-arn ${{ steps.alb-info.outputs.ec2_tg_arn }} \
              --targets Id=$IP,Port=80 \
              --region ${{ env.AWS_REGION }} || true
          done
        fi
        
        echo "‚úÖ Both EC2 and EKS are now active"

    - name: Verify target health
      run: |
        echo "üè• Checking target health..."
        
        # Check EC2 targets
        echo "EC2 Target Group:"
        aws elbv2 describe-target-health \
          --target-group-arn ${{ steps.alb-info.outputs.ec2_tg_arn }} \
          --region ${{ env.AWS_REGION }} \
          --query 'TargetHealthDescriptions[*].[Target.Id, TargetHealth.State]' \
          --output table
        
        # Check EKS targets if exists
        if [ -n "${{ steps.alb-info.outputs.eks_tg_arn }}" ]; then
          echo ""
          echo "EKS Target Group:"
          aws elbv2 describe-target-health \
            --target-group-arn ${{ steps.alb-info.outputs.eks_tg_arn }} \
            --region ${{ env.AWS_REGION }} \
            --query 'TargetHealthDescriptions[*].[Target.Id, TargetHealth.State]' \
            --output table
        fi

    - name: Get ALB URL
      run: |
        ALB_DNS=$(aws elbv2 describe-load-balancers \
          --region ${{ env.AWS_REGION }} \
          --query "LoadBalancers[?contains(LoadBalancerName, 'tx01-alb-${{ inputs.environment }}')].DNSName" \
          --output text)
        echo ""
        echo "üåê Application URL: http://$ALB_DNS"
        echo ""
        echo "Current mode: ${{ inputs.mode }}"

    - name: Summary
      run: |
        echo "### Switch Environment Summary"
        echo ""
        echo "Environment: ${{ inputs.environment }}"
        echo "Mode: ${{ inputs.mode }}"
        echo ""
        
        case "${{ inputs.mode }}" in
          ec2)
            echo "‚úÖ EC2 instances: Active"
            echo "üí§ EKS cluster: Scaled to 0"
            echo "üí∞ Estimated cost: ~\$50/month"
            ;;
          eks)
            echo "üí§ EC2 instances: Stopped"
            echo "‚úÖ EKS cluster: Active"
            echo "üí∞ Estimated cost: ~\$138/month"
            ;;
          both)
            echo "‚úÖ EC2 instances: Active"
            echo "‚úÖ EKS cluster: Active"
            echo "üí∞ Estimated cost: ~\$188/month"
            ;;
        esac
