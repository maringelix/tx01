# MigraÃ§Ã£o EKS - Guia Completo

## ğŸ“‹ VisÃ£o Geral

Este guia descreve o processo de migraÃ§Ã£o da aplicaÃ§Ã£o TX01 de EC2 para Amazon EKS (Elastic Kubernetes Service), mantendo o ALB, RDS e Secrets Manager existentes.

## ğŸ¯ Objetivos

- âœ… Provisionar cluster EKS com node groups gerenciados
- âœ… Reutilizar infraestrutura existente (ALB, RDS, Secrets Manager, ECR)
- âœ… Permitir switch entre EC2 e EKS para otimizaÃ§Ã£o de custos
- âœ… Implementar auto-scaling horizontal de pods (HPA)
- âœ… Manter zero downtime durante operaÃ§Ãµes

## ğŸ’° Comparativo de Custos

### Ambiente Staging

| Componente | EC2 Mode | EKS Mode | Both Active |
|------------|----------|----------|-------------|
| Compute | 2x t2.micro ($16) | EKS Control Plane ($73) + 2x t3.small ($30) | $119 |
| ALB | $16 | $16 | $16 |
| RDS | $13 | $13 | $13 |
| NAT Gateway | $32 | $32 | $32 |
| Data Transfer | ~$5 | ~$8 | ~$8 |
| **Total/mÃªs** | **~$82** | **~$172** | **~$188** |

> **Nota**: Custos reais podem variar. EKS oferece melhor escalabilidade e gerenciamento.

## ğŸ—ï¸ Arquitetura

### Arquitetura HÃ­brida (EC2 + EKS)

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Route 53 /    â”‚
                          â”‚  CloudFront     â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚       ALB       â”‚
                          â”‚  (Compartilhado)â”‚
                          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                               â”‚      â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Target Group  â”‚                   â”‚  Target Group  â”‚
        â”‚      EC2       â”‚                   â”‚      EKS       â”‚
        â”‚  Priority: 50  â”‚                   â”‚  Priority: 50  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  2x EC2 t2.microâ”‚                  â”‚  EKS Cluster   â”‚
        â”‚  + Docker       â”‚                  â”‚  2x t3.small   â”‚
        â”‚  Container      â”‚                  â”‚  + Pods        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                                     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  RDS PostgreSQL â”‚
                     â”‚  (Compartilhado)â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Secrets Manager â”‚
                     â”‚  (Compartilhado)â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Componentes Criados

### 1. Terraform Module (terraform/modules/eks.tf)

**Recursos provisionados:**
- EKS Cluster (v1.28)
- EKS Node Group (managed, auto-scaling)
- IAM Roles (cluster + nodes)
- Security Groups com regras para ALB
- OIDC Provider para IRSA
- IAM Role para AWS Load Balancer Controller
- CloudWatch Log Group

### 2. Kubernetes Manifests (k8s/)

| Arquivo | DescriÃ§Ã£o |
|---------|-----------|
| `deployment.yaml` | Define pods da aplicaÃ§Ã£o com health checks e resource limits |
| `service.yaml` | ClusterIP service para comunicaÃ§Ã£o interna |
| `ingress.yaml` | Ingress com anotaÃ§Ãµes para ALB Controller |
| `hpa.yaml` | Horizontal Pod Autoscaler (2-10 replicas, CPU 70%, Memory 80%) |
| `serviceaccount.yaml` | Service Account com IRSA para Secrets Manager |
| `secret.yaml` | Kubernetes secret com credenciais do RDS |

### 3. GitHub Actions Workflows

#### eks-deploy.yml

**Actions suportadas:**
- `provision`: Cria cluster EKS e instala Load Balancer Controller
- `deploy`: Deploy da aplicaÃ§Ã£o no EKS
- `destroy`: Remove cluster EKS

**Steps principais:**
1. Terraform apply com `enable_eks=true`
2. InstalaÃ§Ã£o do AWS Load Balancer Controller via Helm
3. CriaÃ§Ã£o de secrets no Kubernetes
4. Deploy de Deployment, Service, Ingress e HPA
5. VerificaÃ§Ã£o de health dos pods

#### switch-environment.yml

**Modos suportados:**
- `ec2`: Ativa apenas EC2, para EKS (replicas=0)
- `eks`: Ativa apenas EKS, desliga EC2 instances
- `both`: Ambos ativos com load balancing 50/50

**Funcionamento:**
1. Identifica Target Groups (EC2 e EKS)
2. Ajusta prioridades das rules no ALB listener
3. Escala deployments (EC2 start/stop, EKS scale)
4. Valida health dos targets

## ğŸš€ Processo de MigraÃ§Ã£o

### Passo 1: Provisionar EKS Cluster

```bash
# Via GitHub Actions
1. Ir em Actions â†’ EKS Deploy
2. Selecionar:
   - environment: stg
   - action: provision
3. Run workflow

# Ou via Terraform local
cd terraform/stg
terraform plan -var="enable_eks=true"
terraform apply -var="enable_eks=true"
```

**Tempo estimado**: 15-20 minutos (criaÃ§Ã£o do cluster)

**ValidaÃ§Ã£o**:
```bash
aws eks list-clusters --region us-east-1
aws eks describe-cluster --name tx01-eks-stg --region us-east-1
```

### Passo 2: Deploy da AplicaÃ§Ã£o no EKS

```bash
# Via GitHub Actions
1. Ir em Actions â†’ EKS Deploy
2. Selecionar:
   - environment: stg
   - action: deploy
3. Run workflow
```

**O que acontece:**
1. Configura kubectl com credenciais do cluster
2. Cria ECR registry secret
3. Busca credenciais do RDS no Secrets Manager
4. Cria Kubernetes secret com DB credentials
5. Aplica Deployment (2 pods iniciais)
6. Aplica Service (ClusterIP)
7. Aplica Ingress (cria Target Group automaticamente)
8. Aplica HPA (auto-scaling)
9. Aguarda pods ficarem ready

**Tempo estimado**: 3-5 minutos

**ValidaÃ§Ã£o**:
```bash
kubectl get pods
kubectl get svc
kubectl get ingress
kubectl get hpa
kubectl logs -l app=tx01
```

### Passo 3: Testar EKS em Paralelo com EC2

Neste ponto, ambos estarÃ£o ativos (mode: both):

```bash
# Via GitHub Actions
1. Ir em Actions â†’ Switch Environment
2. Selecionar:
   - environment: stg
   - mode: both
3. Run workflow
```

**ValidaÃ§Ã£o**:
- Acessar ALB DNS: http://tx01-alb-stg-XXXXXXXXXX.us-east-1.elb.amazonaws.com
- Refreshar vÃ¡rias vezes - deve alternar entre EC2 e EKS (load balancing)
- Verificar mÃ©tricas no CloudWatch

### Passo 4: Migrar TrÃ¡fego para EKS

Quando estiver confiante:

```bash
# Via GitHub Actions
1. Ir em Actions â†’ Switch Environment
2. Selecionar:
   - environment: stg
   - mode: eks
3. Run workflow
```

**O que acontece:**
1. Escala EKS deployment para 2 replicas
2. Aguarda pods ficarem healthy
3. Para EC2 instances (stop, nÃ£o terminate)
4. Ajusta prioridade do ALB: EKS=50, EC2=100

**Rollback rÃ¡pido (se necessÃ¡rio)**:
```bash
# Voltar para EC2
1. Ir em Actions â†’ Switch Environment
2. Selecionar mode: ec2
3. Run workflow (leva ~2 minutos)
```

## ğŸ” Monitoramento

### MÃ©tricas do EKS

```bash
# Pods
kubectl top pods
kubectl get hpa

# Logs
kubectl logs -l app=tx01 --tail=100 -f

# Events
kubectl get events --sort-by='.lastTimestamp'
```

### CloudWatch

- **Log Group**: `/aws/eks/tx01-eks-stg/cluster`
- **MÃ©tricas**: EKS cluster logs (API, audit, authenticator)

### ALB Target Health

```bash
aws elbv2 describe-target-health \
  --target-group-arn <TG_ARN> \
  --region us-east-1
```

## ğŸ› ï¸ Troubleshooting

### Pods nÃ£o iniciam

```bash
# Ver motivo
kubectl describe pod <POD_NAME>

# Logs do pod
kubectl logs <POD_NAME>

# Eventos do namespace
kubectl get events
```

**Causas comuns:**
- Image pull error: Verificar ECR secret
- CrashLoopBackOff: Verificar env vars (DB credentials)
- Pending: Verificar resources (CPU/memory)

### ConexÃ£o com RDS falha

```bash
# Verificar secret
kubectl get secret tx01-db-credentials -o yaml

# Verificar se pod tem credenciais
kubectl exec -it <POD_NAME> -- env | grep DB_

# Testar conectividade
kubectl exec -it <POD_NAME> -- nc -zv <RDS_ENDPOINT> 5432
```

**SoluÃ§Ãµes:**
1. Verificar security group do RDS permite EKS subnets
2. Verificar se secret foi criado corretamente
3. Verificar SSL estÃ¡ habilitado no cÃ³digo

### ALB nÃ£o roteia para EKS

```bash
# Verificar ingress
kubectl get ingress -o yaml

# Verificar se target group foi criado
aws elbv2 describe-target-groups --region us-east-1 | grep k8s-default-tx01

# Verificar targets registrados
aws elbv2 describe-target-health --target-group-arn <TG_ARN>
```

**SoluÃ§Ãµes:**
1. Verificar Load Balancer Controller estÃ¡ rodando: `kubectl get pods -n kube-system | grep aws-load-balancer`
2. Verificar annotations do ingress
3. Verificar logs do controller: `kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller`

### HPA nÃ£o escala

```bash
# Verificar mÃ©tricas disponÃ­veis
kubectl top pods

# Verificar HPA status
kubectl describe hpa tx01-hpa
```

**Causas comuns:**
- Metrics Server nÃ£o instalado (EKS jÃ¡ vem com ele)
- Pods sem resource requests definidos
- CPU/memory abaixo do threshold

## ğŸ”’ SeguranÃ§a

### IRSA (IAM Roles for Service Accounts)

O Service Account `tx01-sa` pode assumir IAM role para acessar:
- Secrets Manager
- ECR
- CloudWatch Logs (futuro)

### Network Policies

Considerar implementar Network Policies para:
- Restringir trÃ¡fego entre namespaces
- Permitir apenas pods especÃ­ficos acessarem RDS
- Bloquear egress nÃ£o autorizado

### Pod Security Standards

RecomendaÃ§Ãµes:
- NÃ£o rodar containers como root
- Usar read-only filesystem
- Definir security context
- Scan de vulnerabilidades nas imagens

## ğŸ’¡ OtimizaÃ§Ãµes Futuras

### 1. Spot Instances para Nodes

```hcl
capacity_type = "SPOT"
instance_types = ["t3.small", "t3a.small", "t2.small"]
```

**Economia**: AtÃ© 70% no custo dos nodes

### 2. Cluster Autoscaler

Instalar Cluster Autoscaler para ajustar nÃºmero de nodes automaticamente:

```bash
helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm install cluster-autoscaler autoscaler/cluster-autoscaler \
  --set autoDiscovery.clusterName=tx01-eks-stg \
  --set awsRegion=us-east-1
```

### 3. Fargate para Jobs

Para workloads temporÃ¡rios, considerar Fargate:
- Sem gerenciamento de nodes
- Pay-per-pod (mais caro, mas serverless)

### 4. Service Mesh (Istio/LinkerD)

Para observabilidade avanÃ§ada:
- Circuit breaking
- Retry policies
- Traffic splitting (canary deployments)
- Distributed tracing

### 5. GitOps com ArgoCD

Deploy contÃ­nuo com ArgoCD:
- Sync automÃ¡tico do Git para cluster
- Rollback visual
- Multi-cluster management

## ğŸ“Š Checklist de ValidaÃ§Ã£o

Antes de migrar produÃ§Ã£o:

- [ ] EKS cluster criado e healthy
- [ ] Pods startam sem erros
- [ ] Conectividade com RDS OK
- [ ] ALB roteia trÃ¡fego para EKS
- [ ] Health checks passando
- [ ] HPA funciona (testar com carga)
- [ ] Logs disponÃ­veis no CloudWatch
- [ ] MÃ©tricas visÃ­veis (CPU, memory, network)
- [ ] Rollback para EC2 testado
- [ ] DocumentaÃ§Ã£o atualizada
- [ ] Time treinado nos comandos kubectl

## ğŸ“ Comandos Ãšteis

```bash
# Context do cluster
aws eks update-kubeconfig --name tx01-eks-stg --region us-east-1

# Ver todos recursos
kubectl get all

# Logs em tempo real
kubectl logs -l app=tx01 -f --tail=50

# Executar comando no pod
kubectl exec -it <POD> -- /bin/sh

# Port forward (debug local)
kubectl port-forward svc/tx01-service 8080:80

# Escalar manualmente
kubectl scale deployment tx01-app --replicas=5

# Restart deployment (zero downtime)
kubectl rollout restart deployment tx01-app

# Ver histÃ³rico de deploys
kubectl rollout history deployment tx01-app

# Rollback para versÃ£o anterior
kubectl rollout undo deployment tx01-app

# Deletar pod especÃ­fico (serÃ¡ recriado)
kubectl delete pod <POD_NAME>
```

## ğŸ“ Suporte

Em caso de problemas:

1. Verificar logs: `kubectl logs -l app=tx01`
2. Verificar events: `kubectl get events --sort-by='.lastTimestamp'`
3. Verificar GitHub Actions logs
4. Verificar CloudWatch Logs
5. Consultar [TROUBLESHOOTING.md](TROUBLESHOOTING.md)

---

**Ãšltima atualizaÃ§Ã£o**: $(date +%Y-%m-%d)
**VersÃ£o**: 1.0.0
**Autor**: DevOps Team
